{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Text Generation\n",
    "\n",
    "Other then the tasks mentioned previously, new uprising area in computer vision is image generation and synthesis. \n",
    "\n",
    "## Image understanding\n",
    "\n",
    "So far tried to understand what is in an image. But there are cases we need to understand what is happening inside of an image (like interactions between objects). There are several techniques we can use for that and some of them are as below.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "In early Vision applications, usage of last hidden layer values as embedding was popular. This is easy to handle since embeddings comes as a secondary output of training process. But it has 2 major issues.\n",
    "\n",
    "1. To create this type of embedding, it is needed to have a very large dataset. Also embeddings will only work in categories found in the trained dataset.\n",
    "\n",
    "2. These Embeddings will discard most of the information provided in the image since it will only consider the labels provided for each of the training image. So embeddings will not contain details that are unrelated to original task model was trained on.\n",
    "\n",
    "\n",
    "### Auxiliary Learning Tasks\n",
    "\n",
    "In general embedding generation, we use a typical vision model and get the outputs from a last layer. But this is not always possible to do. So there's another possible method named auxiliary learning. Idea behind this method is to train a model related to completely different task and then use its outputs for something else (ie embeddings). \n",
    "For example, in text we can use the model parameters of word prediction models as a embeddings (word2vec). In computer vision we can use autoencoders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoders\n",
    "\n",
    "AutoEncoders are type of ML architecture which remnisent an hour glass. In this architecture, data we input goes through a bottleneck to build a small vector and then it expands back to the original input. So in a way original input becomes a label to itself. (Can be considered as sloppy compression technique for images). \n",
    "\n",
    "<center><image src=\"./imgs/25.jpg\" width=\"500px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tough part of autoencoders come when we try to identify the correct latent space representation. If it is too small, data will get lost and if it is too big unnecessary data may get captured. So it is up to the creator to experiment and identify the best value.\n",
    "\n",
    "### Variational AutoEncoders\n",
    "\n",
    "One of the main problems in auto-encoder type architecture is that, it does not consider similarities/correlations in latent space. For example, assume we trained a autoencoder to embed circles, squares and triangle images. Then if we randomly generate 2 very close vectors in the latent space and run them through the autoencoder, we would assume the generated images(through decoder) to be of similar type with slight variations. But that is not the case. Since the latent space is not regularized/contrained it does not care about such requirements. Also we would expect the autoencoder would understand the merged behaviour of shapes given the proper inputs. But that's not the case as well. (autoencoders are not trained for enforce to get such organisation: the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised)\n",
    "\n",
    "To mitigate those issues and improve image generation using decoder part, Variational AutoEncoders(VAEs) were developed. They are generative type of model which can cluster together similar training samples rather than creating boundaries between sections like in general autoencoders. \n",
    "\n",
    "In VAEs, the both main problems of AEs have fixed. So now latent space points have a better meaning and model can generate plausible images rather than total random noises.\n",
    "\n",
    "\n",
    "This has been achieved by instead of predicting a vector in latent space, VAEs predict a plausible distribution. Then decoder sample this distribution to recreate the input sample.\n",
    "\n",
    "<center><image src=\"./imgs/26.png\" width=\"500px\"/></center>\n",
    "\n",
    "For more details read this [excellent article](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from implementation stand point, most of the code remains the same, except after the encoder part we expect 3 outputs distribution mean, variance and using those 2 values a random sampled vector. Check an example for better understanding.\n",
    "\n",
    "Another difference is the loss function used in VAEs. It contains a new loss term called Kullback-Leibler divergence which essentially penalize the model for not having a normal distribution as outputs(for encoders output). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
