{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Text Generation\n",
    "\n",
    "Other then the tasks mentioned previously, new uprising area in computer vision is image generation and synthesis. \n",
    "\n",
    "## Image understanding\n",
    "\n",
    "So far tried to understand what is in an image. But there are cases we need to understand what is happening inside of an image (like interactions between objects). There are several techniques we can use for that and some of them are as below.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "In early Vision applications, usage of last hidden layer values as embedding was popular. This is easy to handle since embeddings comes as a secondary output of training process. But it has 2 major issues.\n",
    "\n",
    "1. To create this type of embedding, it is needed to have a very large dataset. Also embeddings will only work in categories found in the trained dataset.\n",
    "\n",
    "2. These Embeddings will discard most of the information provided in the image since it will only consider the labels provided for each of the training image. So embeddings will not contain details that are unrelated to original task model was trained on.\n",
    "\n",
    "\n",
    "### Auxiliary Learning Tasks\n",
    "\n",
    "In general embedding generation, we use a typical vision model and get the outputs from a last layer. But this is not always possible to do. So there's another possible method named auxiliary learning. Idea behind this method is to train a model related to completely different task and then use its outputs for something else (ie embeddings). \n",
    "For example, in text we can use the model parameters of word prediction models as a embeddings (word2vec). In computer vision we can use autoencoders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoders\n",
    "\n",
    "AutoEncoders are type of ML architecture which remnisent an hour glass. In this architecture, data we input goes through a bottleneck to build a small vector and then it expands back to the original input. So in a way original input becomes a label to itself. (Can be considered as sloppy compression technique for images). \n",
    "\n",
    "<center><image src=\"./imgs/25.jpg\" width=\"500px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tough part of autoencoders come when we try to identify the correct latent space representation. If it is too small, data will get lost and if it is too big unnecessary data may get captured. So it is up to the creator to experiment and identify the best value.\n",
    "\n",
    "### Variational AutoEncoders\n",
    "\n",
    "One of the main problems in auto-encoder type architecture is that, it does not consider similarities/correlations in latent space. For example, assume we trained a autoencoder to embed circles, squares and triangle images. Then if we randomly generate 2 very close vectors in the latent space and run them through the autoencoder, we would assume the generated images(through decoder) to be of similar type with slight variations. But that is not the case. Since the latent space is not regularized/contrained it does not care about such requirements. Also we would expect the autoencoder would understand the merged behaviour of shapes given the proper inputs. But that's not the case as well. (autoencoders are not trained for enforce to get such organisation: the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised)\n",
    "\n",
    "To mitigate those issues and improve image generation using decoder part, Variational AutoEncoders(VAEs) were developed. They are generative type of model which can cluster together similar training samples rather than creating boundaries between sections like in general autoencoders. \n",
    "\n",
    "In VAEs, the both main problems of AEs have fixed. So now latent space points have a better meaning and model can generate plausible images rather than total random noises.\n",
    "\n",
    "\n",
    "This has been achieved by instead of predicting a vector in latent space, VAEs predict a plausible distribution. Then decoder sample this distribution to recreate the input sample.\n",
    "\n",
    "<center><image src=\"./imgs/26.png\" width=\"500px\"/></center>\n",
    "\n",
    "For more details read this [excellent article](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from implementation stand point, most of the code remains the same, except after the encoder part we expect 3 outputs distribution mean, variance and using those 2 values a random sampled vector. Check an example for better understanding.\n",
    "\n",
    "Another difference is the loss function used in VAEs. It contains a new loss term called Kullback-Leibler divergence which essentially penalize the model for not having a normal distribution as outputs(for encoders output). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting application of computer vision is image generation. This is relatively new area of research that is being improved. Below are few models that has developed for image generation purposes.\n",
    "\n",
    "### Generative Adversarial Networks\n",
    "\n",
    "One of the generative types of model introduced is GANs. There idea is to have 2 adversarial(competitive) neural network to generate and classify data to build new images. Those 2 networks are `generator` which generate the image and `discriminator` which classify the images whether they are real or generated types. \n",
    "\n",
    "<center><image src=\"./imgs/27.png\" width=\"500px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see an example implementation of generative adversarial network in [here](https://github.com/DinushkaDDS/DeepLearning-Exersices/tree/master/GANs).\n",
    "\n",
    "But due to its inherent nature, GANs are extremely hard to train in practice for real world applications. Therefore, researches were done to find improvements for GANs. \n",
    "\n",
    "One such improvment is, deep convolutional GANs. Instead of using generic dense layers, in these type of GAN researcher have used convolutional layers. This change improve the image generation quality, but still does not solve other problems like convergance.\n",
    "\n",
    "Another one is conditional GANs. In GANs, we have no control over the output it deliver. It will just generate an output based on the random input vector. But this is not useful for us to do in practical applications. Instead, we can provide extra information to the GAN as a condition to generate what we need. Since during the training phase we can provide labels (as a embedding vector) to input data, we can effectively make the GAN to produce targetted values. Technically speaking both generator/discriminator parts of GAN is slightly different than the original implementation. Therefore it is recommended to read it in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image to Image Translation\n",
    "\n",
    "Image to image translation means converting an image from one domain to another (target domain. For example image of horses can be converted in to zebras. To achieve the mentioned task, a architecture called CycleGAN was introduced. Other than that, there are several other models like PatchGAN, Pix2Pix discriminator etc to make more robust image translations. \n",
    "\n",
    "\n",
    "### Super Resolution\n",
    "\n",
    "Super resolution is a technique to improve/upsample low quality images to a better version. Even though this has been a hot topic, not until recent generative models it became popular. With architectures like SRGAN (Super Resolution GAN) DL models are able to create perpetually correct images with proper details. \n",
    "\n",
    "\n",
    "### Inpainting\n",
    "\n",
    "Inpainting is a process which can be used to fix an image with realistic segments. One of the prominent application of this technique is google pixel phone's object removal tool.\n",
    "\n",
    "To achieve this we can first remove the segment we want from the input image and then send the rest to a encoder decoder network. Then we will train the encoder/decoder network to infer the missing part by itself (hopefully). Above part can be considered as the Generator segment of a GAN network. Then we can use the output of that generator and use a discriminator model to classify real/fake images. \n",
    "\n",
    "### Anomaly Detection\n",
    "\n",
    "This is another application of GANs, where we can use to identify whether a given image is anomalous or not. Some examples for usecases of this are counterfeit currrency detection, tumors in medical images etc.\n",
    "\n",
    "To train a model to detect anomalies, it is important to train the model only on normal data. The idea is if an input image does not fit the expected normal data distribution, then it must be a anomaly.\n",
    "\n",
    "### Deepfakes\n",
    "\n",
    "Deepfakes are types of application which can generate fake images based on real inputs. For example we can swap out person X face from an original image to another person Y. \n",
    "\n",
    "To achieve that there are several techniques available. One method is to use one encoder and 2 decoders. So lets say we have image of person X and we want to substitute person Y face there. We first take person X image distorted and send it through the encoder and decoder A. Here we try to recreate the original image of X using decoder A and the common encoder. Also we train the other decoder Busing person Y images(distorted) in same way. Once we did that, we will have 2 encoder decoder networks specialized for regenerating person X and Y images from distorted inputs separately. Then now if we send distored person X image through decoder B, it will think that input is related to person Y and will try to restore the image. Hence a fake image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
