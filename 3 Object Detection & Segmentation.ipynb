{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection and Segmentation\n",
    "\n",
    "Other than the typical classification related usecases, some of the main other vision use cases are object detection and segmentation. Object detection is identifying the objects in a image. Segmentation means separating out image elements. This can be instance level separation or whole image separation. These 2 types are called instance segmentation and whole-scene semantic segmentation.\n",
    "\n",
    "## Object detection\n",
    "\n",
    "Unlike image classification, where we only try to detect whether there's an object, here we try to locate that object in the image. But as you we can already see, object classification already need to locate the object before classification. Problem is those algorithms does not return the object location information. So to make use of that, easiest solution is to predict bounding boxes arorund the detected object. That's in fact the idea behind 'YOLO' approach. \n",
    "\n",
    "Unfortunately using typical network architectures does not yield the best result for object detection. Therefore researchers introduced a new architecture called `Feature Pyramid Networks or FPNs` and illustrated their usage with RetinaNet.\n",
    "\n",
    "\n",
    "### **YOLO (You Only Look Once)**\n",
    "\n",
    "This is one of the simplest object detection architectures with fastest prediction times. Due to this reason it is used in many real time applications. Also this has now been evolved in to many other versions like YOLO v2 - v5 as well with slight differences to the architecture for better prediction result and performance improvements.\n",
    "\n",
    "Basic idea behind YOLO is that it divides an image to NxM cells and tries to predict a bounding box for an object that may be in those cells. Bounding box may not necessarilly be same size as the initial cell. But predicted box center should be inside of the considering cell. \n",
    "\n",
    "<center><image src=\"./imgs/8.png\" width=\"400px\"/></center>\n",
    "\n",
    "Then for each of these grid cell, YOLO will output below values.\n",
    "\n",
    "    x: center x of the detected object in that cell (tanh activation)\n",
    "    y: center y of the detected object in that cell (tanh activation)\n",
    "    w: width of the detected object as ratio (sigmoid activation)\n",
    "    h: width of the detected object as ratio (sigmoid activation)\n",
    "    c: confidence for the detected object\n",
    "    class: this is a softmax for the classes we train our model\n",
    "\n",
    "<center><image src=\"./imgs/9.png\" width=\"600px\"/></center>\n",
    "\n",
    "Note in the above image feature map is for a single data input (For a single image). So single vertical column depicts the outputs related to single grid cell.\n",
    "\n",
    "As you can see due to this reason, it is important to make sure that final feature map have correct dimentions so that we can get the desired representation of output. One way to achieve this is carefully design the ML network to get the required dimensions. But typically this is cumbersome to achieve. Instead authors suggested to flatten the 3D feature map and then send it through a fully connected layer with size (grid_w x grid_h x (5 + numclasses)) and then reshape it to our requirement. Apparently this yields better results compared to directly using the feature map.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
