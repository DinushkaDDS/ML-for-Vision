{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection and Segmentation\n",
    "\n",
    "Other than the typical classification related usecases, some of the main other vision use cases are object detection and segmentation. Object detection is identifying the objects in a image. Segmentation means separating out image elements. This can be instance level separation or whole image separation. These 2 types are called instance segmentation and whole-scene semantic segmentation.\n",
    "\n",
    "## Object detection\n",
    "\n",
    "Unlike image classification, where we only try to detect whether there's an object, here we try to locate that object in the image. But as you we can already see, object classification already need to locate the object before classification. Problem is those algorithms does not return the object location information. So to make use of that, easiest solution is to predict bounding boxes arorund the detected object. That's in fact the idea behind 'YOLO' approach. \n",
    "\n",
    "Unfortunately using typical network architectures does not yield the best result for object detection. Therefore researchers introduced a new architecture called `Feature Pyramid Networks or FPNs` and illustrated their usage with RetinaNet.\n",
    "\n",
    "---\n",
    "\n",
    "### **YOLO (You Only Look Once)**\n",
    "\n",
    "This is one of the simplest object detection architectures with fastest prediction times. Due to this reason it is used in many real time applications. Also this has now been evolved in to many other versions like YOLO v2 - v5 as well with slight differences to the architecture for better prediction result and performance improvements.\n",
    "\n",
    "Basic idea behind YOLO is that it divides an image to NxM cells and tries to predict a bounding box for an object that may be in those cells. Bounding box may not necessarilly be same size as the initial cell. But predicted box center should be inside of the considering cell. \n",
    "\n",
    "<center><image src=\"./imgs/8.png\" width=\"400px\"/></center>\n",
    "\n",
    "Then for each of these grid cell, YOLO will output below values.\n",
    "\n",
    "    x: center x of the detected object in that cell (tanh activation)\n",
    "    y: center y of the detected object in that cell (tanh activation)\n",
    "    w: width of the detected object as ratio (sigmoid activation)\n",
    "    h: width of the detected object as ratio (sigmoid activation)\n",
    "    c: confidence for the detected object\n",
    "    class: this is a softmax for the classes we train our model\n",
    "\n",
    "<center><image src=\"./imgs/9.png\" width=\"600px\"/></center>\n",
    "\n",
    "Note in the above image feature map is for a single data input (For a single image). So single vertical column depicts the outputs related to single grid cell.\n",
    "\n",
    "As you can see due to this reason, it is important to make sure that final feature map have correct dimentions so that we can get the desired representation of output. One way to achieve this is carefully design the ML network to get the required dimensions. But typically this is cumbersome to achieve. Instead authors suggested to flatten the 3D feature map and then send it through a fully connected layer with size (grid_w x grid_h x (5 + numclasses)) and then reshape it to our requirement. Apparently this yields better results compared to directly using the feature map.\n",
    "\n",
    "To build the loss function for this we need training data with ground truth boxes and their classes. So then we can compare our output predicted boxes against the ground truth boxes. Also loss function need to consider classification results as well. This is straight forward if a grid cell predicts a single box. But this is not the case in practice. Because most of the time there may be cases some objects may space across multiple grid cells making them overlap. In these cases having one prediction box per cell is not enough. To solve this problem, in YOLO architecture there's a parameter to define number of bounding boxes per a grid cell. This means in the output there will be more x,y,w,h,c values. Check the below image.\n",
    "\n",
    "<center><image src=\"./imgs/10.png\" width=\"400px\"/></center>\n",
    "\n",
    "So with these new additions, comparing the prediction with ground truth becomes bit more complex. In fact, instead of just directly comparing each prediction with the ground truth, YOLO uses a metric called 'IOU' or Intersection over Union between all the ground truth boxes and all predicted boxes within a grid cell. Then it select the pairing with highest IOU as the correct prediction.\n",
    "\n",
    "<center><image src=\"./imgs/11.png\" width=\"400px\"/></center>\n",
    "\n",
    "With this in place, Total loss per a sample would be a combination of below values.\n",
    "\n",
    "1. **Object Presence Loss:**\n",
    "        Each cell that has a ground truth box, compute (1-C)^2. (Loss for not correctly predicting the object presence).\n",
    "2. **Object Absence Loss:**\n",
    "        Each cell that does not have a ground truth box computes C^2. (Loss for predicting a object which is not available)\n",
    "3. **Object classification Loss:**\n",
    "        Typical cross entropy loss (Loss for incorrectly classifying the detected object).\n",
    "4. **Bounding Box Loss**\n",
    "        Loss for bad bounding box dimension prediction.\n",
    "\n",
    "Then YOLO combine all above losses with different weighting factors to optimize the model for object detection.\n",
    "\n",
    "One of the  main limitation of the YOLO architecture is that it can only predict single class per grid cell(can select multiple boxes per grid cells though). So will not work if there are multilple items in single grid cell.\n",
    "\n",
    "Another issue is grid itself. This grid cell separation makes the model not invariant to scale. So unless we carefully tune the model, it will not be able to identify small objects.\n",
    "\n",
    "Also YOLO tends to locate objects with low precision. This may be because YOLO tries to identify the bounding box from the last feature map of the network. (Usually at such level, spacial details of the original image tend to not be in the last feature map)\n",
    "\n",
    "---\n",
    "\n",
    "### **RetinaNet**\n",
    "\n",
    "Compared to the YOLOv1, this has several new concepts used in its architecture and in its loss calculation. Their architecture design include feature pyramid networks which combine information extracted in multiple scales as well. \n",
    "\n",
    "#### **Feature Pyramid Maps**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Yolo architecture, detection part only used the last high level feature maps that comes after all the processings of the convolutional layers. This causes the model to be less accurate in its localization capability as the high level features have less spacial resolution by design. \n",
    "\n",
    "To circumvent this problem, researchers looked in to a way to combine early level feature maps into detection head inputs hoping that would help to improve the localization capabilities. Thus feature pyramid networks were born. Check the below diagram.\n",
    "\n",
    "<center><image src=\"./imgs/12.png\" width=\"500px\"/></center>\n",
    "\n",
    "As we can see in Feature pyramid networks high level feature maps get upsampled and transfered back to low level. Then such combined feature maps will be used in detection tasks. \n",
    "\n",
    "<center><image src=\"./imgs/13.png\" width=\"500px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the FPN, high level feature maps get upsampled and then get added up to the low level feature maps. This helps the spacial features to get preserved while having fine tuned fearures. Then such combined feature maps will get send to the detection heads to produce the detection box and classification. The point to note is that, channel size of inputs to the detection head. Since all these inputs get fed into same (shared) detection head it is important to have same depth size. (Other dimensions can be managed because of the convolutional operations)\n",
    "\n",
    "Also FPN does not depend on the convolutional network it is being used as long as it can access the intermediate feature maps. So it can be used with any major networks like Resnets or Efficient nets. \n",
    "\n",
    "\n",
    "#### **Anchor Boxes**\n",
    "\n",
    "In YOLO architecture, detections were happened considering a predefined base grid. and trying to minimize that grid to fit the objects. Instead some architectures expanded this idea by predefining a set of 'anchor boxes' with various aspect ratios and scales. Then these boxes were used as the bases for the detection boxes. Idea behind this is to make the model to predict values around zero (Since objects can be in various places anchor boxes only need to have small adjustments compared to grids). This helps neural network activation function to be more actively predict the output values. But this would also face some of the problems encountered in the YOLO grid based detection (like more objects than the boxes etc). So this value need to be setted up based on the requirement and need to be in various sizes and aspect ratios for best results.\n",
    "\n",
    "In the context of RetinaNet there are nine types of anchor box types in various scales and aspect ratios. Those along with the feature maps computed by the feature pyramid network become the inputs to the final prediction tasks. In summary RetinaNet sequence of operations for predicting bounding boxes are as follows.\n",
    "\n",
    "1. Feature pyramid network reduce the image in to feature maps.\n",
    "2. Each map will be used to predict the bounding boxes related to the anchors. For example 4 * 6 (* 256 channels) feature map will have 24 (4 * 6) anchor locations.\n",
    "\n",
    "<center><image src=\"./imgs/14.png\" width=\"400px\"/></center>\n",
    "\n",
    "3. The detection head part uses multiple convolutional layers to convert the input feature maps to final output which should have (num_anchor_boxes_per_location * 4) channels. Those additional 4 values per box provides the deltas to center(x, y), width and height of predicted bounding box compared to the anchor box.\n",
    "\n",
    "4. Like wise each feature map from Pyramid network will use different scales of anchor boxes to predict bounding boxes. \n",
    "\n",
    "This different scaling of anchor boxes in each feature pyramid levels should be harmonize with the input image size. For example in RetinaNet there are 5 feature maps (p1, p2, p3, p4, p5) and for each feture map 2^n times small anchor boxes used compared to the input image where n is the feature map depth (eg:- p3 feature map --> n=3).\n",
    "\n",
    "> Due to above mentioned reasons it is important to finetune the anchor box sizes and scales or resize input images to match the corresponding detection tasks.\n",
    "\n",
    "When calculating the detection loss, we need to take different approach compared to early models. But the generic loss calculation ideas remain the same. Before the loss calculation we first need to assign ground truth boxes to best anchor.\n",
    "\n",
    "To do that, we assign the anchor boxes to the ground truth boxes which have the highest IOU value.So we first calculate all the pairwise IOU between anchors and ground truths. Then we arrange those in a matrix format with N (number of ground truth boxes) rows and M (number of anchor boxes) as columns.\n",
    "\n",
    "Then this matrix is analyzed by columns under below conditions.\n",
    "\n",
    "- an anchor is assigned to ground truth that has the largest IOU in its column provided it is greater than 0.5. \n",
    "- IOU should be greater than 0.4 or it is considered as detected nothing.\n",
    "\n",
    "Once ground truths are paired with anchors, model can go ahead with the computing box predictions, classification and losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "The outputs(Feature maps) from the FPNs are used by the detection head and classification head to predict the class of the object and it's bounding box deltas. As mentioned above feature maps are 3 dimensional where their x, y are spacial and other being number of channels.\n",
    "\n",
    "In RetinaNet for every spacial location for every feature map several parameters will get predicted.\n",
    "\n",
    "B: number of anchor box types (Retina net this is 9)\n",
    "K: number of classes\n",
    "\n",
    "1. The prediction head predicts the class probabilities (B * K), for every anchor point.\n",
    "2. The detection head predicts the (B * 4) bounding box deltas (explained in previous section). \n",
    "\n",
    "These 2 heads have same type of design and will evaluate feature maps from all the scales sent from the FPN.\n",
    "\n",
    "<center><image src=\"./imgs/15.png\" width=\"200px\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Focal Loss for Classification\n",
    "\n",
    "When we think about the number of anchor points for a input image it get adds up to considerable level of number considering we need to have anchor points for several levels of feature maps. Also we need to have multiple anchor boxes per anchor point as well. This causes large number of boxes to do predictions. But in practice one input image would only have very small amount of ground truth boxes.\n",
    "\n",
    "This creates a problem when calculating the error. Since there are so many background detecting anchor boxes, errors calculated from those can reduce the effectiveness of the boxes that detected the actual objects. To fix this issue, RetinaNet Paper suggested an impressive solution. They tweaked the loss function to produce much smaller values on empty backgrounds. This technique is called \"Focal Loss\".\n",
    "\n",
    "As we can see in the image provided in the above section, classification head uses sigmoid functions to calculate the final outputs. So for all classes they used the binary cross entropy losses. The loss formula for single class predicted probability p, actual label value y would be like below.\n",
    "\n",
    "`Cross Entropy(y, p) = − y ⋅log(p) − (1 − y)⋅log(1 − p)`\n",
    "\n",
    "The modification of above to get the Focal loss formula is as follows.\n",
    "\n",
    "`Focal Loss(y, p) = − y ⋅ (1 − p)^γ ⋅log(p) − (1 − y) ⋅p^γ ⋅log(1 − p)`\n",
    "\n",
    "When the parameter gamma (γ) is 0, this get simplfied to the base cross entropy formula. But in other case it's behaviour is bit different. In the case of y==0 (which means ground truth background) if we plot the Focal loss to the probability values for various gamma values we will be able to see that the loss is getting exponentially reduced to lower levels. This way model can safely add up errors from all the anchor boxes and still train the model to perform better. (And another reason why our mathematic knowledge should be very good. We can utilize these kinds of cool tricks to solve our problems!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smooth L1 loss for Bounding box error\n",
    "\n",
    "The detection head does regression as its objective is to provide values for the delta values. The normal way of calculating the error in regression tasks is L1(absolute error) and L2(squared error) loss. Both of these have their own strengths and weaknesses, so usage need to be thought accordingly. But in the case of RetinaNet authors choosed the middle ground of both which is called `Huber Loss` or `Smooth L1 loss`.\n",
    "\n",
    "This loss calculation method behaves like L2 loss for small values and L1 loss to large values. This helps the model to unnecessarily generate large errors while maintaining good convergance ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Maximum Suppression\n",
    "\n",
    "As you might(should) probably wondering, since many anchor boxes can be on top of a detected object, once all the classifications and identifications done there can be multiple bounding boxes per single identified object. \n",
    "\n",
    "<center><image src=\"./imgs/16.png\" width=\"200px\"/></center>\n",
    "\n",
    "To fix that issue we can use the technique `Non Maximum Suppression`. Here what happens is it takes the detected box IOU and its confidence to find the most appropriate bounding box. Its very simple to implement without any optimizations like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST A PSUEDO CODE\n",
    "\n",
    "def NMS(boxes, class_probabilities_per_box, IOU_THRES = 0.5):\n",
    "    '''\n",
    "        This only consider bounding box for a single class\n",
    "    '''\n",
    "\n",
    "    result_boxes = []\n",
    "\n",
    "    for b1 in boxes:\n",
    "        non_useful = False\n",
    "\n",
    "        for b2 in boxes:\n",
    "            if (IOU(b1, b2) > IOU_THRES):\n",
    "                if(class_probabilities_per_box[b2]>class_probabilities_per_box[b1]):\n",
    "                    non_useful = True\n",
    "        if(not non_useful):\n",
    "            result_boxes.append(b1)\n",
    "\n",
    "    return result_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty simple logic which is very useful in practice, but can have some unwanted problems as well. One example is if there are 2 objects in close range which got detected, but have a overlapping IOU more than the defined threshold, then the object with lower confidence will get ignored. To fix such issues there's a variant of NMS called `Soft NMS` which continuously decrease the confidence of overlapping bounding boxes. This helps to reduce the scenarios of objects getting missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
