{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection and Segmentation\n",
    "\n",
    "Other than the typical classification related usecases, some of the main other vision use cases are object detection and segmentation. Object detection is identifying the objects in a image. Segmentation means separating out image elements. This can be instance level separation or whole image separation. These 2 types are called instance segmentation and whole-scene semantic segmentation.\n",
    "\n",
    "## Object detection\n",
    "\n",
    "Unlike image classification, where we only try to detect whether there's an object, here we try to locate that object in the image. But as you we can already see, object classification already need to locate the object before classification. Problem is those algorithms does not return the object location information. So to make use of that, easiest solution is to predict bounding boxes arorund the detected object. That's in fact the idea behind 'YOLO' approach. \n",
    "\n",
    "Unfortunately using typical network architectures does not yield the best result for object detection. Therefore researchers introduced a new architecture called `Feature Pyramid Networks or FPNs` and illustrated their usage with RetinaNet.\n",
    "\n",
    "---\n",
    "\n",
    "### **YOLO (You Only Look Once)**\n",
    "\n",
    "This is one of the simplest object detection architectures with fastest prediction times. Due to this reason it is used in many real time applications. Also this has now been evolved in to many other versions like YOLO v2 - v5 as well with slight differences to the architecture for better prediction result and performance improvements.\n",
    "\n",
    "Basic idea behind YOLO is that it divides an image to NxM cells and tries to predict a bounding box for an object that may be in those cells. Bounding box may not necessarilly be same size as the initial cell. But predicted box center should be inside of the considering cell. \n",
    "\n",
    "<center><image src=\"./imgs/8.png\" width=\"400px\"/></center>\n",
    "\n",
    "Then for each of these grid cell, YOLO will output below values.\n",
    "\n",
    "    x: center x of the detected object in that cell (tanh activation)\n",
    "    y: center y of the detected object in that cell (tanh activation)\n",
    "    w: width of the detected object as ratio (sigmoid activation)\n",
    "    h: width of the detected object as ratio (sigmoid activation)\n",
    "    c: confidence for the detected object\n",
    "    class: this is a softmax for the classes we train our model\n",
    "\n",
    "<center><image src=\"./imgs/9.png\" width=\"600px\"/></center>\n",
    "\n",
    "Note in the above image feature map is for a single data input (For a single image). So single vertical column depicts the outputs related to single grid cell.\n",
    "\n",
    "As you can see due to this reason, it is important to make sure that final feature map have correct dimentions so that we can get the desired representation of output. One way to achieve this is carefully design the ML network to get the required dimensions. But typically this is cumbersome to achieve. Instead authors suggested to flatten the 3D feature map and then send it through a fully connected layer with size (grid_w x grid_h x (5 + numclasses)) and then reshape it to our requirement. Apparently this yields better results compared to directly using the feature map.\n",
    "\n",
    "To build the loss function for this we need training data with ground truth boxes and their classes. So then we can compare our output predicted boxes against the ground truth boxes. Also loss function need to consider classification results as well. This is straight forward if a grid cell predicts a single box. But this is not the case in practice. Because most of the time there may be cases some objects may space across multiple grid cells making them overlap. In these cases having one prediction box per cell is not enough. To solve this problem, in YOLO architecture there's a parameter to define number of bounding boxes per a grid cell. This means in the output there will be more x,y,w,h,c values. Check the below image.\n",
    "\n",
    "<center><image src=\"./imgs/10.png\" width=\"400px\"/></center>\n",
    "\n",
    "So with these new additions, comparing the prediction with ground truth becomes bit more complex. In fact, instead of just directly comparing each prediction with the ground truth, YOLO uses a metric called 'IOU' or Intersection over Union between all the ground truth boxes and all predicted boxes within a grid cell. Then it select the pairing with highest IOU as the correct prediction.\n",
    "\n",
    "<center><image src=\"./imgs/11.png\" width=\"400px\"/></center>\n",
    "\n",
    "With this in place, Total loss per a sample would be a combination of below values.\n",
    "\n",
    "1. **Object Presence Loss:**\n",
    "        Each cell that has a ground truth box, compute (1-C)^2. (Loss for not correctly predicting the object presence).\n",
    "2. **Object Absence Loss:**\n",
    "        Each cell that does not have a ground truth box computes C^2. (Loss for predicting a object which is not available)\n",
    "3. **Object classification Loss:**\n",
    "        Typical cross entropy loss (Loss for incorrectly classifying the detected object).\n",
    "4. **Bounding Box Loss**\n",
    "        Loss for bad bounding box dimension prediction.\n",
    "\n",
    "Then YOLO combine all above losses with different weighting factors to optimize the model for object detection.\n",
    "\n",
    "One of the  main limitation of the YOLO architecture is that it can only predict single class per grid cell(can select multiple boxes per grid cells though). So will not work if there are multilple items in single grid cell.\n",
    "\n",
    "Another issue is grid itself. This grid cell separation makes the model not invariant to scale. So unless we carefully tune the model, it will not be able to identify small objects.\n",
    "\n",
    "Also YOLO tends to locate objects with low precision. This may be because YOLO tries to identify the bounding box from the last feature map of the network. (Usually at such level, spacial details of the original image tend to not be in the last feature map)\n",
    "\n",
    "---\n",
    "\n",
    "### **RetinaNet**\n",
    "\n",
    "Compared to the YOLOv1, this has several new concepts used in its architecture and in its loss calculation. Their architecture design include feature pyramid networks which combine information extracted in multiple scales as well. \n",
    "\n",
    "#### **Feature Pyramid Maps**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Yolo architecture, detection part only used the last high level feature maps that comes after all the processings of the convolutional layers. This causes the model to be less accurate in its localization capability as the high level features have less spacial resolution by design. \n",
    "\n",
    "To circumvent this problem, researchers looked in to a way to combine early level feature maps into detection head inputs hoping that would help to improve the localization capabilities. Thus feature pyramid networks were born. Check the below diagram.\n",
    "\n",
    "<center><image src=\"./imgs/12.png\" width=\"500px\"/></center>\n",
    "\n",
    "As we can see in Feature pyramid networks high level feature maps get upsampled and transfered back to low level. Then such combined feature maps will be used in detection tasks. \n",
    "\n",
    "<center><image src=\"./imgs/13.png\" width=\"500px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the FPN, high level feature maps get upsampled and then get added up to the low level feature maps. This helps the spacial features to get preserved while having fine tuned fearures. Then such combined feature maps will get send to the detection heads to produce the detection box and classification. The point to note is that, channel size of inputs to the detection head. Since all these inputs get fed into same (shared) detection head it is important to have same depth size. (Other dimensions can be managed because of the convolutional operations)\n",
    "\n",
    "Also FPN does not depend on the convolutional network it is being used as long as it can access the intermediate feature maps. So it can be used with any major networks like Resnets or Efficient nets. \n",
    "\n",
    "\n",
    "#### **Anchor Boxes**\n",
    "\n",
    "In YOLO architecture, detections were happened considering a predefined base grid. and trying to minimize that grid to fit the objects. Instead some architectures expanded this idea by predefining a set of 'anchor boxes' with various aspect ratios and scales. Then these boxes were used as the bases for the detection boxes. Idea behind this is to make the model to predict values around zero (Since objects can be in various places anchor boxes only need to have small adjustments compared to grids). This helps neural network activation function to be more actively predict the output values. But this would also face some of the problems encountered in the YOLO grid based detection (like more objects than the boxes etc). So this value need to be setted up based on the requirement and need to be in various sizes and aspect ratios for best results.\n",
    "\n",
    "In the context of RetinaNet there are nine types of anchor box types in various scales and aspect ratios. Those along with the feature maps computed by the feature pyramid network become the inputs to the final prediction tasks. In summary RetinaNet sequence of operations for predicting bounding boxes are as follows.\n",
    "\n",
    "1. Feature pyramid network reduce the image in to feature maps.\n",
    "2. Each map will be used to predict the bounding boxes related to the anchors. For example 4 * 6 (* 256 channels) feature map will have 24 (4 * 6) anchor locations.\n",
    "\n",
    "<center><image src=\"./imgs/14.png\" width=\"400px\"/></center>\n",
    "\n",
    "3. The detection head part uses multiple convolutional layers to convert the input feature maps to final output which should have (num_anchor_boxes_per_location * 4) channels. Those additional 4 values per box provides the deltas to center(x, y), width and height of predicted bounding box compared to the anchor box.\n",
    "\n",
    "4. Like wise each feature map from Pyramid network will use different scales of anchor boxes to predict bounding boxes. \n",
    "\n",
    "This different scaling of anchor boxes in each feature pyramid levels should be harmonize with the input image size. For example in RetinaNet there are 5 feature maps (p1, p2, p3, p4, p5) and for each feture map 2^n times small anchor boxes used compared to the input image where n is the feature map depth (eg:- p3 feature map --> n=3).\n",
    "\n",
    "> Due to above mentioned reasons it is important to finetune the anchor box sizes and scales or resize input images to match the corresponding detection tasks.\n",
    "\n",
    "When calculating the detection loss, we need to take different approach compared to early models. But the generic loss calculation ideas remain the same. Before the loss calculation we first need to assign ground truth boxes to best anchor.\n",
    "\n",
    "To do that, we assign the anchor boxes to the ground truth boxes which have the highest IOU value.So we first calculate all the pairwise IOU between anchors and ground truths. Then we arrange those in a matrix format with N (number of ground truth boxes) rows and M (number of anchor boxes) as columns.\n",
    "\n",
    "Then this matrix is analyzed by columns under below conditions.\n",
    "\n",
    "- an anchor is assigned to ground truth that has the largest IOU in its column provided it is greater than 0.5. \n",
    "- IOU should be greater than 0.4 or it is considered as detected nothing.\n",
    "\n",
    "Once ground truths are paired with anchors, model can go ahead with the computing box predictions, classification and losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
