{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection and Segmentation\n",
    "\n",
    "## Image Segmentation\n",
    "\n",
    "\n",
    "Object classification focused on making categories and detection focused on looking for an object in a given scene. Separate to the above requirements there are use cases where we need to separate the items/objects of a scene to various classes rather than just recognizing and finding the bounding box. The name of that process is called segmentation. What basically happen here is all the pixels of the scene will get assigned a unique identifier to separate out same object and background.\n",
    "\n",
    "---\n",
    "\n",
    "### **Masked R-CNN and Instance Segmentation**\n",
    "\n",
    "Earlier we discussed about the YOLO and RetinaNet models. These models are called single shot detectors as they only need to traverse the image once to detect the required object. Other than this approach there is another technique which use 2 neural networks. It uses the first neural network to suggest a potential location for the object to be detected, then use the second network to clssify and fine tune the boundary boxes. This type of architecture is called `Region Proposal Networks (RPNs)`.\n",
    "\n",
    "But as we can imagine RPNs tend to be more complex and slower compared to the single shot detectors. On the other hand they are more accurate as well. Because of that reason there are many variant of RPNs like R-CNN, fast R-CNN and faster R-CNN. One of the interesting such RPN type model is masked R-CNN. \n",
    "\n",
    "One main reason for considering this model is not because it perform slightly better compared to previously mentioned RPNs, but its ability to perform instance segmentation tasks with slight extensions. In fact it can be extended to find every pixel belonging to the detected object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposal Networks (RPNs)\n",
    "\n",
    "RPN is a type of simplified single shot detection network that only cares about 2 classes the object (foreground or anything that is labeled in the dataset) and background. It can use an architecture similar to the RetinaNet and will predict the bounding boxes and their classification (object or background) using the 2 prediction heads.\n",
    "\n",
    "It will have a slightly modified loss calculation function since we only care about 2 main classes object or background. Also just as in the RetinaNet predicted boxes will go through non max suppression process. And the left boxes are considered as the box proposal or Region Of Interests (ROIs) for the next stage which is the actual segmentation. \n",
    "\n",
    "It should also be noted that selected ROIs from this stage can still be rejected by the next stage as background, so having perfect accuracy is not essential. Also we can use any network that can predict classification and rough ROI for the object of interest which will be refined in the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN\n",
    "\n",
    "The basic idea of the R-CNN is to get the ROIs cropped from original input and then rerun it through the backbone model to get classifications to the selected objects. But in practice this is slow/redundand process and therefore instead we directly use the related feature maps of the ROIs. This way we can avoid the rerunning the cropped image parts separately through the backbone network. This process get bit more complicated if we are using FPNs. Since ROIs can come from different levels we need to choose the proper FPN level. Also the authors of Mask R-CNN paper noted that using rounded values for the feature map widths and heights of FPN may degrade the performance of the model.\n",
    "\n",
    "<center><image src=\"./imgs/18.png\" width=\"500px\"/></center>\n",
    "\n",
    "Once we figure out the feature maps for the ROIs rest is straight forward. We just need to send the extracted feature through the predictions heads to get the classifications and precise box refinements. Loss calculation and other related parts are almost similar to the RetinaNet.\n",
    "\n",
    "The difference of Masked R-CNN is it has a third classification head to classify the individual pixels of objects. The result is a pixel mask that outline the object. But to train this part we need training data with objects masks as well. Before going in to more details of the masked head, we need to look at special operation used in Masked R-CNN for pixel generation called `Transposed Convolutions`.\n",
    "\n",
    "#### Transposed Convolutions\n",
    "\n",
    "In simple terms transposed convolutions is an upsampling operation. Difference between this technique and regular upsampling techniques is that former having learnable weights. \n",
    "\n",
    "<center><image src=\"./imgs/17.png\" width=\"400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above image, for every input value (in the top) will get multiplied by a filter and applied to the output. The stride defined in the filter movement makes the upsampling happen. Also if input have multiple channels filter would also have that number of channels and do the operation. Then will take the element wise summation before adding to the final output. If we need to have multiple output channels then we need to have multiple filters.\n",
    "\n",
    "> It should be noted that this technique is known to cause checker board like artifacts in the output. Therefore later researchers introduced `up-convolutions` technique which have a basic neighbour sampling followed by a typical convolution layer that can reduce the mentioned issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Segmentation\n",
    "\n",
    "The early part of the model used classification heads to decide whether a bounding box is background or foreground and other finetune the bounding box. But segmentation requires to assign pixels to each identified object. To do that there is another prediction head in Masked R-CNN architecture. This can be bit confusing at this point, but below is a simplified diagram for the overall architecture.\n",
    "\n",
    "<center>K: Number of classes, N: Number of ROIs proposed by RPN<image src=\"./imgs/19.png\" width=\"500px\"/></center>\n",
    "\n",
    "Idea behind this confusing architecture is that, since mask prediction head only have to work on a ROI which has a high probability to have a interesting object it can have a higher precision. In the R-CNN implementation, it basically upsamples the ROI feature maps that come from the RPN using the transposed convolutions described above and output a black and white image that matches the detected object region.\n",
    "\n",
    "One interesting point to note in the above architecture is that, in the mask head it will produce one mask for each class for each of the predictions. It is easy to note that this step is redundant since we already have a classification head to say what type of object we are expecting. We can simply make the mask head to predict one mask according to the classification heads prediction. But in practice, this redundand type mask prediction helps the mask head to learn class specific hints about objects, and hence the design choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few other practical choices made by the Mask R-CNN researchers like having more channels for the mask head, pixel by pixel cross entropy for mask loss calculation etc. But those falls into actual implementation details which I will not go through. Also as we can see Mask R-CNN predicted object masks are low resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **U-Net and Semantic Segmentation**\n",
    "\n",
    "In instance segmentation what we tried to achieve was identifying each object separately and mark them. In semantic segmentation we try to mark everything of same class into a single segment. For an example all the people in a image would be considered as single segment class.\n",
    "\n",
    "For this type of usecase U-Net can be taken as a simple sufficient solution for most of the usecases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
