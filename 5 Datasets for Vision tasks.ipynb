{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for Computer Vision tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any computer vision task, the most important preliminary work is to find a proper dataset. Based on the task we are trying to achieve there are many types of vision data types available.\n",
    "\n",
    "Normal images are the most basic type of data for computer vision. These images can be taken from social medias, scans or other sources. Could be taken by normal person or professional photographer. Could be taken in a controlled environment or in uncontrolled environment.\n",
    "\n",
    "Depending on the quality of images as researchers or engineers we have to make adjustments to our models and systems.\n",
    "\n",
    "- Larger images will require large memory capacity to train ML models.\n",
    "- Larger images means high number of parameters to train.\n",
    "- High resolution images tend to have more noise near low light environments.\n",
    "- High resolution images take larger cost to store and transmit.\n",
    "\n",
    "To find good size images to ML work, best approach is to find the highest resolution that is adequate to our problem while keeping the other resource contraints in check.\n",
    "\n",
    "Other than the normal images we use in daily works there are some other types of data also available for computer vision tasks. \n",
    "\n",
    "- Many instruments like X-Ray, MRI, LIDARS etc create a 2D/3D images of a space. Most of the time they contain one channel and therefore can consider as a greyscale image. In CT scans what it returns is set of 3D slices of the area. We can consider that as multiple channels during calculations.\n",
    "\n",
    "- One interesting type of vision data is polar grids. This type of data get created from Radar/ Ultrasound sensors. They have a general cone shape due to how the sensor works and therefore before using in ML applications we need to modify its content to be more meaningful for practical applications. One method is unwrapping the content. Other is just using the polar grid as it is with additional channel indicating how far a point is from the center.\n",
    "\n",
    "<center><image src=\"./imgs/21.png\" width=\"300px\"/></center>\n",
    "\n",
    "- In case of geospacial data like land ownership, topography, population density we can take all those as several channels of single image assuming data is taken over same area/projection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But all above image types are essentially very similar. But there are some cases we can use typical Vision ml techniques to 1D or 3D data structures like videos and audio data.\n",
    "\n",
    "### Spectrogram\n",
    "\n",
    "To do machine learning on audio, we can first split the data to chunks and then apply ML to those chunks.\n",
    "On the other hand Audio is a 1D signal, so it is possible to use Conv1D operations in place of Conv2D. This would be like processing audio signal in time space. However in practice better to represent audio as spectrogram(A stacked view of spectrums of frequencies in the audio signal varies over time.). So in simple terms spectogram x axis shows time and y axis shows the frequency. The pixel value represent the spectral density(loudness) of the audio signal at the specific frequency.\n",
    "\n",
    "<center><image src=\"./imgs/22.png\" width=\"500px\"/></center>\n",
    "\n",
    "> Representing audio signal in above format provides an interesting capability from ML perspective. Since now the audio signal is 2D image, now we can use Computer vision techniques and models to process audio. In fact this type of technique can be used in NLP problems as well. We can convert text in to embeddings and form a 2D representation. Then we can apply ML vision algorithms on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos\n",
    "\n",
    "Obviously since videos consist of continuous image frame, we can just simply apply ML techniques to each frame and do our task specially classification and object identification like tasks.\n",
    "On the other hand we can process multiple frames at a time as a rolling average and then apply ML algorithms. To do that we can use 3D convolutions. Also we can use RNNs and other sequencial processing methods(Attention) to apply ML to videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
