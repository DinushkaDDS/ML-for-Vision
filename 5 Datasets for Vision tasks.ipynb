{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets for Computer Vision tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any computer vision task, the most important preliminary work is to find a proper dataset. Based on the task we are trying to achieve there are many types of vision data types available.\n",
    "\n",
    "Normal images are the most basic type of data for computer vision. These images can be taken from social medias, scans or other sources. Could be taken by normal person or professional photographer. Could be taken in a controlled environment or in uncontrolled environment.\n",
    "\n",
    "Depending on the quality of images as researchers or engineers we have to make adjustments to our models and systems.\n",
    "\n",
    "- Larger images will require large memory capacity to train ML models.\n",
    "- Larger images means high number of parameters to train.\n",
    "- High resolution images tend to have more noise near low light environments.\n",
    "- High resolution images take larger cost to store and transmit.\n",
    "\n",
    "To find good size images to ML work, best approach is to find the highest resolution that is adequate to our problem while keeping the other resource contraints in check.\n",
    "\n",
    "Other than the normal images we use in daily works there are some other types of data also available for computer vision tasks. \n",
    "\n",
    "- Many instruments like X-Ray, MRI, LIDARS etc create a 2D/3D images of a space. Most of the time they contain one channel and therefore can consider as a greyscale image. In CT scans what it returns is set of 3D slices of the area. We can consider that as multiple channels during calculations.\n",
    "\n",
    "- One interesting type of vision data is polar grids. This type of data get created from Radar/ Ultrasound sensors. They have a general cone shape due to how the sensor works and therefore before using in ML applications we need to modify its content to be more meaningful for practical applications. One method is unwrapping the content. Other is just using the polar grid as it is with additional channel indicating how far a point is from the center.\n",
    "\n",
    "<center><image src=\"./imgs/21.png\" width=\"300px\"/></center>\n",
    "\n",
    "- In case of geospacial data like land ownership, topography, population density we can take all those as several channels of single image assuming data is taken over same area/projection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But all above image types are essentially very similar. But there are some cases we can use typical Vision ml techniques to 1D or 3D data structures like videos and audio data.\n",
    "\n",
    "### Spectrogram\n",
    "\n",
    "To do machine learning on audio, we can first split the data to chunks and then apply ML to those chunks.\n",
    "On the other hand Audio is a 1D signal, so it is possible to use Conv1D operations in place of Conv2D. This would be like processing audio signal in time space. However in practice better to represent audio as spectrogram(A stacked view of spectrums of frequencies in the audio signal varies over time.). So in simple terms spectogram x axis shows time and y axis shows the frequency. The pixel value represent the spectral density(loudness) of the audio signal at the specific frequency.\n",
    "\n",
    "<center><image src=\"./imgs/22.png\" width=\"500px\"/></center>\n",
    "\n",
    "> Representing audio signal in above format provides an interesting capability from ML perspective. Since now the audio signal is 2D image, now we can use Computer vision techniques and models to process audio. In fact this type of technique can be used in NLP problems as well. We can convert text in to embeddings and form a 2D representation. Then we can apply ML vision algorithms on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos\n",
    "\n",
    "Obviously since videos consist of continuous image frame, we can just simply apply ML techniques to each frame and do our task specially classification and object identification like tasks.\n",
    "On the other hand we can process multiple frames at a time as a rolling average and then apply ML algorithms. To do that we can use 3D convolutions. Also we can use RNNs and other sequencial processing methods(Attention) to apply ML to videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling the Datasets\n",
    "\n",
    "Labeling the dataset is one of the first tasks almost every data science team have to do in their new works. Depending on the target we need to achive we need to have labeling in different manner.\n",
    "\n",
    "For classification like task most common way to label data is by either the folder structure or using a metadata table. The problem in using folders for data labeling is that it can lead to data duplication if a image have multiple labels. Therefore in such situation metadata table is preferred.\n",
    "\n",
    "When we are labeling data for object detection task, we need to store the data regarding the bounding box dimensions. To do that we can have an additional field in the metadata table.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling at scale\n",
    "\n",
    "In many ML usecases it is required to have considerable sized dataset to train the model. Therefore to build up a dataset to such purpose in efficient and accurate manner, there are several tools and techniques available.\n",
    "\n",
    "> One such tool is the `Computer Vision Annotation Tool`. It is a free, web based image annotation tool that can be installed locally as well.\n",
    "\n",
    "If we need to annotate images to support multiple tasks then it is efficient to use an interactive interface like jupyter notebook. There is a package named `multi-label-pigeon` that can be used in annotation tasks.\n",
    "\n",
    "Install the package using \n",
    "<pre>pip install multi-label-pigeon</pre>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59769196f064afb888dcf18de53b7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='0 examples annotated, 2 examples left')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62d167463e446acb5fe0e17fc36f962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='tulip', style=ButtonStyle()), Button(description='rose', style=ButtonStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a4da7493bd4c74863a43d43608bf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='red', style=ButtonStyle()), Button(description='green', style=ButtonStyle()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f703fca8f64d319b7dd58acd6cf0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='done', style=ButtonStyle()), Button(description='back', style=ButtonStyle()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155f6e69f7554468a91bd0c8562a284e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multi_label_pigeon import multi_label_annotate\n",
    "from IPython.display import display, Image\n",
    "\n",
    "files = ['imgs/rose.jpg']\n",
    "\n",
    "annotations = multi_label_annotate(\n",
    "    files,\n",
    "    options={'flowers':['tulip', 'rose', 'sunflower'], 'color':['red', 'green', 'blue']},\n",
    "    display_fn=lambda filename: display(Image(filename, width=250))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of using the annotation pacakge.\n",
    "\n",
    "Another massive scale labeling method is voting and crowdsourcing. In this technique what it does is images are pushed toward a voting system so that multiple raters can label/tag the given image. Image with consistant tags from multiple raters get assigned with those particular tags, while images with conflicting labels sent to verification or get ignored. \n",
    "\n",
    "## Automated Labeling\n",
    "\n",
    "In many Deep Learning cases we need to have massive amount of labeled data, but it is not possible to manually label each and every data point due to various time and cost constraints. In such scenarios we can use several workaround techniques.\n",
    "\n",
    "One is infering labels from related data. In somecases we can infer the label of sample by only looking at a specific part of data or its context. This way we can easily get the tag of our considering data point.\n",
    "\n",
    "Another method is a model called `Noisy Student`. Idea behind this technique is to use a small ML model to to label large amount of data iteratively. Its general steps are as follows. \n",
    "\n",
    "- Manually label a small dataset\n",
    "- Train a small ML model using the above dataset. (Teacher Model)\n",
    "- Use this model to label large set of data\n",
    "- Then train another model (Student Model) with previous manually labeled dataset and new dataset. (It is important to incorporate dropout and other data augmentation methods to make the model generalizable)\n",
    "- Iterate the above process by making the student model the teacher model.\n",
    "\n",
    "This way we can make a large dataset with relatively low tagging effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
