{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Training Pipelines\n",
    "\n",
    "One of the main parts of any machine learning system is its data ingestion pipeline. The efficient implementation of this part makes the whole system fast in general. In general there are several points we can consider to make the data ingesion efficient.\n",
    "\n",
    "- Storing the data efficiently\n",
    "\n",
    "    For example storing data as individual JPEG types files would be highly inefficient in machine learning perspective. Instead we an use more specialized data stuctures like Tensorflow Records. They are more compressed, are in sizes of 50-100 mb range which can be easily utilized in training pipelines.\n",
    "\n",
    "- Parallelizing the reading of data\n",
    "\n",
    "    Once we have prepared the dataset for our requirement, one other aspect we could improve on is data reading. In general cases we can write code to do it ourselves, but most modern frameworks provide the parallel read capabilities as well. But this may not provide a considerable speed up in many cases. Most of the time the code complexity might be more troublesome than the time IO ops may consume. Not to mention in modern ML models training time is the bottleneck, not the data loading times.\n",
    "\n",
    "- Maximizing the GPU utilization\n",
    "\n",
    "    GPU training time can be considered as one of the most costly factors in large scale machine learning models. Therefore it is important to effiently use the GPU time. The three main factors to note are, \n",
    "    1. Everytime we move data between CPU-GPU, it consume time.\n",
    "    2. GPUs are good at parallel processing(matrices) \n",
    "    3. They have limited memory\n",
    "\n",
    "    Therefore when developing the code for ML pipeline, we need to make sure that data does not go from the GPU to CPU in the middle of processing.\n",
    "\n",
    "## Saving model State\n",
    "\n",
    "After a long training process, we need to save our precious trained model for later use. Just because we saved the code, we does not get the learned knowledge. To do do that, we need to preserve the model state values (Matrix weights etc.) Also we can use this preserved model state for retraining purposes as well.\n",
    "\n",
    "Based on the framework we used to build the model, there are several differences between such model saving mechanisms. But one alternative common for many such frameworks is called Open Neural Network Exchange (ONNX), an open source framework agnostic ML model format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
