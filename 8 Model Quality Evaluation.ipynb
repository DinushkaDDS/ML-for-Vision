{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Quality check and Evaluation\n",
    "\n",
    "Once we build and train a ML model, next important part is to analyze its performance. In business cases no one cares about the how cool your model architecture or techniques you used. All they would care about is overrall model quality and how it would hold up against the real world data.\n",
    "\n",
    "Therefore to analyze a model, there are several solutions introduced by researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "A tool provided by tensorflow to visualize models realtime. It can monitor model training matrics, evaluation matrics and even inferencing details as well. Since this have interactive capabilities, users can simply change values of various parameters and check the model behaviour easily.\n",
    "\n",
    "#### Weight Histograms\n",
    "\n",
    "    This is an interesting graph which shows how the weights and other scaler quantities that have too many values to inspect individually varies over the training period. \n",
    "\n",
    "    If we think about how the weights were initialized in normal neural network, most probably it would be uniform random intialization. But with each training cycle weight values would get slightly changed and therefore overall value distribution would change as well. In genaral we expect the weight values to revolve around a normal distribution (Central Limit Theorem).\n",
    "\n",
    "    So when we have a weight histogram of our model, we can identify odd patterns of weight distributions like more zero values, constant distribution etc. and act to fix those issues.\n",
    "\n",
    "\n",
    "Other than that, tensor board provides functionality to visualize the model graph and also to explore the data as well.\n",
    "\n",
    "To use tensorboard in our model training process (tensorflow keras) we can include it as a callback function for the model compilation. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliced Evaluation\n",
    "\n",
    "In today world Machine learning projects, it is important to keep the fairness to all people. There are cases some minorities get weird outputs in ML applications, which causes huge misunderstanding between general public and researchers. To avoid such cases it is important to analyze the model behaviour on edge cases.\n",
    "\n",
    "Though there are cases dataset does not include all the possible examples, it is not the only cause. Typical ML models are focused on identifying general patterns of the majority. In cases of small subsets of data, model may overlook the specialities of minorities and will bias the majority values. Therefore it is important to implement ML model while keeping track of such instances.\n",
    "\n",
    "Regulatization, early stopping, prunning and quantization like techniques may cause models to have mentioned like behaviours. To identify such issues we can use Sliced Evaluations. Idea behind this technique is to segment the validation dataset and then use those segments to identify the behavioural patterns of ML models. \n",
    "\n",
    "(This is an active area of research in today ML world called fairness.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also there are several techniques used to explain a Model behavior. Below are some summerized details of them.\n",
    "\n",
    "### Local Interpretable Model-Agnostic Explanations (LIME)\n",
    "\n",
    "In this technique, it first identifies the patches of image that consist of contigous similar pixels and then replace those with a uniform value essentially removing them. Then that changed image will be sent to the model to do a prediction.\n",
    "\n",
    "The prediction probabilities come out for the modified images get spacially weighted based on how similar they are to original images and LIME present modified images with highest probability weightages as the explanation.\n",
    "\n",
    "Read more details [here](https://homes.cs.washington.edu/~marcotcr/blog/lime/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernalSHAP\n",
    "\n",
    "Tenchnically this is similar to LIME, but it weights the modified patch probabilities differently. KernalSHAP uses image patch predictions on game theory based distribution.\n",
    "\n",
    "Apparently kernalSHAP tend to be computationally expensive compared to LIME but provides better results as well (this is due to the way interpretation of results happen).\n",
    "\n",
    "### Integrated Gradients (IG)\n",
    "\n",
    "An interesting technique based on the idea of gradient value of a given value. For example in an image, pixels with higher importance get higher gradient changes over the training period. IG method tries to identify such changes. But once the model is trained, all we get are trained weights.\n",
    "\n",
    "As mentioned earlier, IG is based on the idea that model will output a theoritical probability of a class given a proper input. So in IG method, we start from a baseline pixel value and continue step by step towards the actual pixel value of the image. While doing that we calculate the overall gradient change happened during the process for all pixels. Then those calculated gradient values are depicted on the original image.\n",
    "\n",
    "<center><image src=\"./imgs/23.png\" width=\"300px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this technique it is important to find a proper baseline image. Otherwise output explained image will not be meaningful. For example it would be not wise to use a total black/white like image for X-RAY image explanation since they have meaningful black regions as proper interesting data points.\n",
    "\n",
    "But in practical usecases IG method does not work as we expect. It tend to highlight image regions with high information regardless of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainable Representations through AI (xRAI)\n",
    "\n",
    "In xRAI method, we try to train an interpretation network based on a trained network. Its aim is to find close approximation of a complex model using a more easy to interpret algebraic expressions.\n",
    "\n",
    "This method combines LIME method preprocessing techniques and KernalSHAP to find patches. Then those patches get run through a IG like mechanism to identify importance of them. Then those patches will be combined with each other based on the closeness of their respective gradient values and such combined regions will be considered for the actual ranking of image region importance.\n",
    "\n",
    "More details can be found in [Google Vertex AI documentation](https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
